{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNclm4NTQbmhXmoP5HjafMD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6u0vC2d-ozH","executionInfo":{"status":"ok","timestamp":1713087656328,"user_tz":-330,"elapsed":10223,"user":{"displayName":"Akash T","userId":"09835383845484366323"}},"outputId":"12bd2b22-a539-4f89-c69c-25a1f55bae97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 3s 3s/step - loss: 1.0982 - accuracy: 0.4000\n","Epoch 2/10\n","1/1 [==============================] - 0s 17ms/step - loss: 1.0938 - accuracy: 0.8000\n","Epoch 3/10\n","1/1 [==============================] - 0s 18ms/step - loss: 1.0904 - accuracy: 0.8000\n","Epoch 4/10\n","1/1 [==============================] - 0s 22ms/step - loss: 1.0873 - accuracy: 0.8000\n","Epoch 5/10\n","1/1 [==============================] - 0s 18ms/step - loss: 1.0841 - accuracy: 0.8000\n","Epoch 6/10\n","1/1 [==============================] - 0s 14ms/step - loss: 1.0808 - accuracy: 0.8000\n","Epoch 7/10\n","1/1 [==============================] - 0s 17ms/step - loss: 1.0775 - accuracy: 0.8000\n","Epoch 8/10\n","1/1 [==============================] - 0s 15ms/step - loss: 1.0742 - accuracy: 0.8000\n","Epoch 9/10\n","1/1 [==============================] - 0s 16ms/step - loss: 1.0707 - accuracy: 0.6000\n","Epoch 10/10\n","1/1 [==============================] - 0s 14ms/step - loss: 1.0671 - accuracy: 0.6000\n","1/1 [==============================] - 1s 648ms/step\n","Predicted emoji: ðŸ˜Š\n"]}],"source":["\n","import numpy as np\n","import tensorflow as tf\n","\n","# Define the dataset\n","X_train = np.array([\n","    \"I love machine learning\",\n","    \"I hate studying\",\n","    \"This is amazing\",\n","    \"I am sad\",\n","    \"This is terrible\"\n","])\n","y_train = np.array([\n","    [1, 0, 0],  # Happy emoji one-hot encoding\n","    [0, 1, 0],  # Angry emoji one-hot encoding\n","    [1, 0, 0],  # Happy emoji one-hot encoding\n","    [0, 0, 1],  # Sad emoji one-hot encoding\n","    [0, 1, 0]   # Angry emoji one-hot encoding\n","])\n","\n","# Tokenize the dataset\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","\n","# Pad sequences to make them uniform\n","X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train)\n","\n","# Define the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32),\n","    tf.keras.layers.LSTM(64),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes: Happy, Angry, Sad\n","])\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=10)\n","\n","# Test the model\n","test_sentence = \"I am happy\"\n","test_sequence = tokenizer.texts_to_sequences([test_sentence])\n","padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(test_sequence)\n","predicted_vector = model.predict(padded_sequence)[0]\n","\n","# Decode the predicted vector to emoji\n","emojis = [\"ðŸ˜Š\", \"ðŸ˜¡\", \"ðŸ˜¢\"]  # Happy, Angry, Sad\n","predicted_index = np.argmax(predicted_vector)\n","predicted_emoji = emojis[predicted_index]\n","print(\"Predicted emoji:\", predicted_emoji)"]}]}